{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harveyj/aoc/blob/master/3_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWQZY_ldgilv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def enc(c):\n",
        "  if c == '.': return 0\n",
        "  return ord(c) - ord('a') + 1\n",
        "def dec(c):\n",
        "  if c == 0: return '.'\n",
        "  return chr(c + ord('a') - 1)\n",
        "names = raw_names.split('\\n')[1:]\n",
        "N = torch.zeros((27, 27), dtype=torch.int32)\n",
        "for n in names:\n",
        "  for c1, c2 in zip(n, n[1:]):\n",
        "    N[enc(c1)][enc(c2)] += 1\n",
        "def normalize(N):\n",
        "  return N / torch.sum(N, dim=1, keepdim=True)\n",
        "norm = normalize(N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asBaD7m65UMA"
      },
      "outputs": [],
      "source": [
        "xs = []; ys = []\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "WINDOW = 3\n",
        "EMBED_SIZE = 5\n",
        "HIDDEN_LAYER = 200\n",
        "DICT_SIZE = 27\n",
        "\n",
        "for n in names:\n",
        "  padded = '.' * WINDOW + n + '.'\n",
        "  for i in range(len(padded) - WINDOW):\n",
        "    xs.append(list(map(enc, padded[i:i+WINDOW])))\n",
        "    ys.append(enc(padded[i+WINDOW]))\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n",
        "\n",
        "# for i in range(300):\n",
        "#   print(list(map(dec, list(xs[i]))), dec(ys[i]))\n",
        "\n",
        "C = torch.randn([DICT_SIZE, EMBED_SIZE])\n",
        "W1 = torch.randn([WINDOW*EMBED_SIZE, HIDDEN_LAYER])\n",
        "B1 = torch.randn([HIDDEN_LAYER])\n",
        "W2 = torch.randn([HIDDEN_LAYER, DICT_SIZE])\n",
        "B2 = torch.randn([DICT_SIZE])\n",
        "parameters = [C, W1, B1, W2, B2]\n",
        "\n",
        "for p in parameters: p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10000):\n",
        "  ixs = torch.randint(0, xs.shape[0], (32,)) # generate random sample of 32 indices\n",
        "\n",
        "  # Forward\n",
        "  emb = C[xs[ixs]]\n",
        "  h = torch.tanh(emb.view(-1, WINDOW*EMBED_SIZE) @ W1 + B1)\n",
        "  logits = h @ W2 + B2\n",
        "  loss = F.cross_entropy(logits, ys[ixs])\n",
        "  # Backward\n",
        "  for p in parameters: p.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  for p in parameters:\n",
        "    p.data += -0.001 * p.grad\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxdVc-mSQZqq",
        "outputId": "6079cccd-a4dd-498a-e05d-9153d867450d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2218, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loss on entire set\n",
        "emb = C[xs]\n",
        "h = torch.tanh(emb.view(-1, WINDOW*EMBED_SIZE) @ W1 + B1)\n",
        "logits = h @ W2 + B2\n",
        "loss = F.cross_entropy(logits, ys)\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaMDfUsqReP0",
        "outputId": "5ef8a9ba-74df-4e8e-ea8e-96b8da9a03ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3858, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  window = [0,0,0]\n",
        "  c = -1\n",
        "  while c != 0:\n",
        "    emb = C[torch.tensor(window)]\n",
        "    hpreact = emb.view(-1, WINDOW*EMBED_SIZE) @ W1 + B1\n",
        "    h = torch.tanh(hpreact)\n",
        "    logits = h @ W2 + B2\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    ix = torch.multinomial(probs, num_samples=1).item()\n",
        "    print(dec(ix), end='')\n",
        "    window = window[1:] + [ix]\n",
        "    if ix == 0: break\n",
        "  print('')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHdA2XV0NCyt",
        "outputId": "4df9dbdc-471a-4496-a53f-e566bf0b51e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "miki.\n",
            "ali.\n",
            "yamarha.\n",
            "faneee.\n",
            "antoniyn.\n",
            "rocerie.\n",
            "zakarishtyn.\n",
            "lomlya.\n",
            "larla.\n",
            "geltzsaby.\n",
            "mymonean.\n",
            "wibi.\n",
            "bquxn.\n",
            "lunyse.\n",
            "mace.\n",
            "kanayla.\n",
            "tadeele.\n",
            "kira.\n",
            "dhli.\n",
            "yah.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhWBpY9Of3Ix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e248ef-08c1-4418-f227-a2464890a3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-18 18:23:48--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.1’\n",
            "\n",
            "\rnames.txt.1           0%[                    ]       0  --.-KB/s               \rnames.txt.1         100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-02-18 18:23:48 (8.21 MB/s) - ‘names.txt.1’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
        "raw_names = open('names.txt', 'r').read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS1DOPNKdqrk"
      },
      "source": [
        "# Recall\n",
        "### Embeddings\n",
        "* keeping n-character context windows around gets really crazy. one-hot encoding a 27-element array means a context window of 4 is 27**4 inputs = 531441 inputs.\n",
        "* Enter embeddings. Instead of one-hot, map each input token (letter in our case) to an N-dimensional vector, where N is far less than the cardinality of the data. Learn the embeddings during the training\n",
        "* Take the embeddings of your input, concatenate them, feed them into the first layer of the neural network.\n",
        "* Repeat the bengio gradient paper. Embeddings point into an MLP."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiiZ2lm3D2NZotEGmXVsQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}