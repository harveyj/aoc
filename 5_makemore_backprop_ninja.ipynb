{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harveyj/aoc/blob/master/5_makemore_backprop_ninja.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rToK0Tku8PPn"
      },
      "source": [
        "## makemore: becoming a backprop ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sFElPqq8PPp"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChBbac4y8PPq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "x6GhEWW18aCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47fd66b0-d09d-4f49-b6c6-d486984bb45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-23 21:52:01--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-02-23 21:52:01 (8.36 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klmu3ZG08PPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bc91c3-c6bc-4c93-a356-92f7b81534c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCQomLE_8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5549a4b5-0171-438b-af5f-e2b1ee7428ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_zt2QHr8PPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83496a4b-4eed-4529-fe17-9affcb640b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg20-vsg8PPt"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJPU8HT08PPu"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFLjQyT8PPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb7a1f0-e301-458b-81ff-8a461e31199d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY-y96Y48PPv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ofj1s6d8PPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2888f14b-0b42-44a9-e551-4294447b3f66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3524, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.tensor([[1, 0, 0]])\n",
        "test.repeat(3, 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihPBowc-zJmI",
        "outputId": "fe4b65f3-0724-452f-f16e-73500857ac97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0],\n",
              "        [1, 0, 0],\n",
              "        [1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ebEAtnZMHi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO-8aqxK8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292acd8c-3eaf-4897-c730-bef6c85950a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 2.0954757928848267e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 9.604264050722122e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 2.0954757928848267e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "# -----------------\n",
        "import math\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/len(logprobs)\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "\n",
        "dprobs = probs ** -1 * math.log(math.e) * dlogprobs\n",
        "cmp('probs', dprobs, probs)\n",
        "\n",
        "# probs = counts * counts_sum_inv\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdims=True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "\n",
        "dcounts_sum = -1 / counts_sum ** 2 * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "\n",
        "dcounts = torch.ones_like(counts) * dcounts_sum + counts_sum_inv * dprobs\n",
        "cmp('counts', dcounts, counts)\n",
        "\n",
        "dnorm_logits = norm_logits.exp() * dcounts\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "\n",
        "dlogit_maxes = -1 * dnorm_logits.sum(1, keepdim=True)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "\n",
        "\n",
        "dlogits = dnorm_logits.clone() # - (net out the impact on the max, which is dlogit_maxes IFF the element == max\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "cmp('logits', dlogits, logits)\n",
        "\n",
        "dh = dlogits @ W2.T\n",
        "cmp('h', dh, h)\n",
        "\n",
        "dW2 = h.T @ dlogits\n",
        "cmp('W2', dW2, W2)\n",
        "\n",
        "db2 = dlogits.sum(0)\n",
        "cmp('b2', db2, b2)\n",
        "\n",
        "dhpreact = (1.0 - h ** 2) * dh\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "\n",
        "dbngain = dhpreact * bnraw\n",
        "dbngain = dbngain.sum(0 , keepdims=True)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "\n",
        "dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "\n",
        "dbnraw = dhpreact * bngain\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "\n",
        "dbnvar_inv = dbnraw * bndiff\n",
        "dbnvar_inv = dbnvar_inv.sum(0, keepdims=True)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "\n",
        "dbnvar = -(1.58114*10**7)/(1 + 100000*bnvar)**1.5 * dbnvar_inv\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "\n",
        "\n",
        "\n",
        "dbndiff = 2 * bndiff * dbndiff2 + bnvar_inv.sum(0, keepdims=True)*dbnraw\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "\n",
        "# emb = C[Xb] # embed the characters into vectors\n",
        "# embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# bndiff = hprebn - bnmeani\n",
        "# print (dbndiff.shape, hprebn.shape, bnmeani.shape)\n",
        "dbnmeani = -1 * dbndiff.sum(0)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "dhprebn = dbndiff.clone()\n",
        "dhprebn = dbndiff\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "# print (dbnmeani.shape, hprebn.shape, dhprebn.shape)\n",
        "# print(hprebn)\n",
        "# print(dhprebn)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "\n",
        "dembcat = dhprebn @ W1.T\n",
        "cmp('embcat', dembcat, embcat)\n",
        "\n",
        "dW1 = embcat.T @ dhprebn\n",
        "cmp('W1', dW1, W1)\n",
        "\n",
        "db1 = dhprebn.sum(0)\n",
        "cmp('b1', db1, b1)\n",
        "\n",
        "demb = dembcat.view(emb.shape)\n",
        "cmp('emb', demb, emb)\n",
        "\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k, j]\n",
        "    dC[ix] += demb[k,j]\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebLtYji_8PPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d58aac0-8213-4e29-da47-4c0fd8b41f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3523573875427246 diff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gCXbB4C8PPx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabad8fb-397f-488d-bd64-d1363c330d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365],\n",
            "        [ 21.8692,  28.4087, -13.4865,  14.8531, -10.6202,  27.7022,  -8.6377,\n",
            "           4.0050, -19.7832,  -0.3337,   5.2703,   2.4548,   2.5561,  -5.4231,\n",
            "           0.9335, -21.9659, -32.9336, -16.5211, -18.1962,  12.5234,  12.6368,\n",
            "         -10.0838, -10.8472,  23.1572,  15.9212,  -6.5340, -12.1365]],\n",
            "       grad_fn=<SumBackward1>)\n",
            "n 32\n",
            "logits_shape torch.Size([32, 27])\n",
            "yb_shape torch.Size([32])\n",
            "tensor(3.3524, grad_fn=<NegBackward0>)\n",
            "lpindexed torch.Size([32])\n",
            "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
            "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])\n",
            "tensor([-4.1976, -3.1728, -3.5278, -3.3469, -4.0924, -3.5448, -3.1670, -4.1345,\n",
            "        -3.1337, -4.2410, -3.1641, -1.6144, -2.8777, -3.0870, -3.1051, -3.0844,\n",
            "        -3.8502, -2.9689, -3.6167, -3.3687, -2.8867, -2.9388, -4.2708, -4.0510,\n",
            "        -3.5093, -2.8729, -2.9806, -3.8274, -2.7368, -3.4818, -3.3107, -3.1133],\n",
            "       grad_fn=<IndexBackward0>)\n",
            "range(0, 32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 8, 14, 15, 22,  0, 19,  9, 14,  5,  1, 20,  3,  8, 14, 12,  0, 11,  0,\n",
              "        26,  9, 25,  0,  1,  1,  7, 18,  9,  3,  5,  9,  0, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dlogits = None # TODO. my solution is 3 lines\n",
        "print(logits[F.one_hot(Yb)].sum(1))\n",
        "print('n', n)\n",
        "print('logits_shape', logits.shape)\n",
        "print('yb_shape', Yb.shape)\n",
        "print(loss)\n",
        "print('lpindexed', logprobs[range(n), Yb].shape)\n",
        "print(Yb)\n",
        "print(logprobs[range(n), Yb])\n",
        "print(range(n))\n",
        "# H(P, Q) = — sum x in X P(x) * log(Q(x))\n",
        "# So, the derivative of the cross-entropy loss with respect to the predicted probabilities  is simply the difference between the predicted and true probabilities.\n",
        "Yb\n",
        "\n",
        "# logits: 32x27\n",
        "# logprobs = 32x27\n",
        "# Yb: 32\n",
        "# loss: <scalar>\n",
        "# -----------------\n",
        "\n",
        "#cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd-MkhB68PPy"
      },
      "outputs": [],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POdeZSKT8PPy"
      },
      "outputs": [],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = None # TODO. my solution is 1 (long) line\n",
        "# -----------------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPy8DhqB8PPz"
      },
      "outputs": [],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "#with torch.no_grad():\n",
        "\n",
        "# kick off optimization\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmean = hprebn.mean(0, keepdim=True)\n",
        "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "  hpreact = bngain * bnraw + bnbias\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "  # manual backprop! #swole_doge_meme\n",
        "  # -----------------\n",
        "  # YOUR CODE HERE :)\n",
        "  dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
        "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "  # -----------------\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p, grad in zip(parameters, grads):\n",
        "    p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "    #p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEpI0hMW8PPz"
      },
      "outputs": [],
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KImLWNoh8PP0"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aFnP_Zc8PP0"
      },
      "outputs": [],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esWqmhyj8PP1"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHeQNv3s8PP1"
      },
      "outputs": [],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall 2/23/24\n",
        "\n",
        "- Assume a basic understanding of neural nets with fully connected layers modeled by Wx+B\n",
        "- Neural nets are typically trained by dividing input into training, dev, and test sets (80/10/10)\n",
        "- Train on training, eval on dev, use test very sparingly\n",
        "- The pytorch model, gives you n-dimensional \"tensors\" (vectors, matrices)\n",
        "- They overload @ (matrix mul), *, +, - etc operations and store a bunch of metadata about the connections to other tensors (see below)\n",
        "- The typical training pass of any neural net is: initialize the net (with some amount of entropy, the exact training params are a bit of an art)\n",
        "- Run the inputs from the training set (call this Xtrain) through the neural net (this is the \"forward pass\"), this gives you an output at the other side of the net (these are often called \"logits\").\n",
        "- The rough semantics at the logits (output) layer is that you have one neuron per output vocabulary, and the probability of the neuron firing is the probability of that \"word\" from your output (in language models these are tokens) being the output word.\n",
        "- The output layer is often run through a \"softmax\" (this normalizes all of the logits/log-likelihoods into a distribution from 0 to 1.\n",
        "- Now, you have the calculated likelihoods for each of the Xtrain inputs and the true outputs (call this Ytrain). Look up the probability of Ytrain being output in the likelihoods. Take their logs and sum the logs up over the training set. Multiply by -1 and you have the \"negative log likelihood\" which is often called the loss. Minimizing the loss = maximizing the likelihoods = maximizing the accuracy.\n",
        "- Next, do the \"backward pass\", aka \"back propagation\"\n",
        "- You know the chain of mathematical operations that led you to the output, and you know the loss.\n",
        "- Presuming that all of the mathematical operations are differentiable, you can now calculate dLoss/dVAL for every VAL in the chain. this is the \"gradient\".\n",
        "- As we are just chaining adds and multiplies (in general). You do this via calculus's chain rule.\n",
        "- if you have X@A@B@W --> loss, you can calculate dLoss/dW, then calculate dLoss/dB (which is dLoss/dW * dW/dB), and so on.\n",
        "- dLoss/dX now tells you that if you perturb dX by a little bit, how you are influencing Loss.\n",
        "- Now, go over all of the parameters in your model. Adjust them all a *teeny* bit in the direction that will decrease loss.\n",
        "- Repeat the above three steps many many times and see how low you can get the loss.\n",
        "\n",
        "Next level of concepts\n",
        "- \"overfitting\" - in many circumstances you can \"overfit\" which means your model simply \"learns\" to copy-paste your training data. If this happens, running the model against examples that are *not* in your training set is likely to go poorly. As a general rule, if your number of parameters exceeds the size of your training set, you should be wary of this. The best way to combat this in practice is to separate out your data into training and dev/test sets, as mentioned above. If your loss on the training set is lower than your loss on the dev/test sets, that is a good sign that you are beginning to overfit and should stop adding parameters or whatever else you were doing that is causing the overfitting.\n",
        "- \"batching\" - In practice, you do not run the entire data set through the model at once. You split the data set into *many* batches, and run the model many more times over these batches. In practice this leads to much faster convergence.\n",
        "- \"hyperparameters\" - Parameters which determine the structure of the model are called hyperparameters. For example, the number of layers and depth of the model, the width of the model (number of parameters) at various layers, and the learning rate are all hyperparameters.\n",
        "- \"learning rate\" - how much you adjust the model by on each training pass. If you set this rate too high, the model will wildly fluctuate and fail to converge. If you set it too low, it will take far too long for the model to reach an optimal state."
      ],
      "metadata": {
        "id": "IPdaltOKgUiV"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}